---
title: "STAT 420: Final Project"
author: "Daniil Finkel, Omar Boffil, Albert Ferguson"
date: "August 7, 2020"
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


<h2>Predicting UPDRS (Unified Parkinson Disease Rating Scale) from Biomedical Voice Measurements of Individuals with Early Stage Parkinson’s Disease</h2>

<br>

## Introduction

Telehealth is a growing field. Remote medical examinations offer several benefits over conventional on-site methods:

- Secure and confidential: patients do not need to be seen at a specialists office
- Convenience: patients can attend appointments from comfort of their own home
- Increased access: patients are no longer restricted to geographically local physicians
- Reduced cancellation: patients more consistently attend, more closely monitored
- Many others

*Telemonitoring* is one form of telehealth in which information technology is used to monitor patients at a distance. The feasibility of such automated approaches calls for robust diagnoses to justify widespread adoption. This project aims to contribute to that goal. 

Parkinson’s disease is a neurodegenerative disease characterized by stiffness and shakiness in motor functions of affected individuals. The disease tends to become more severe over time.

A research collaboration between the University of Oxford, 10 medical centers in the US, and Intel Corporation organized a 6-month trial, recording a range of biomedical voice measurements from 42 people with early-stage Parkinson's disease. The effort produced a dataset with the following characteristics:

- 5875 observations
- 22 attributes:
  - subject# - Integer that uniquely identifies each subject
  - age - Subject age
  - sex - Subject gender '0' - male, '1' - female
  - test_time - Time since recruitment into the trial. The integer part is the number of days since recruitment.
  - motor_UPDRS - Clinician's motor UPDRS score, linearly interpolated
  - total_UPDRS - Clinician's total UPDRS score, linearly interpolated
  - Jitter(%), Jitter(Abs), Jitter:RAP, Jitter:PPQ5, Jitter:DDP
    - Several measures of variation in fundamental frequency
  - Shimmer, Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, Shimmer:APQ11, Shimmer:DDA
    - Several measures of variation in amplitude
  - NHR, HNR - Two measures of ratio of noise to tonal components in the voice
  - RPDE - A nonlinear dynamical complexity measure
  - DFA - Signal fractal scaling exponent
  - PPE - A nonlinear measure of fundamental frequency variation

The dataset and a description of the aims of it’s collection are provided here: https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring

A key attribute in the above set is `total_UPDRS`. The Unified Parkinson's Disease Rating Scale (UPDRS) is used to measure the degree of Parkinson’s in an individual - a higher score indicating a more severe case.

This project aims to develop a model to predict `total_UPDRS` using the remaining attributes, effectively developing a model to predict the degree to which an individual is affected by Parkinson’s disease.

<br>

**Let's take a closer look and do a little bit of pre-processing on the data:**

<br>

## Methods

### Exploratory Phase

Examining the dataset revealed it was already very clean. There were no empty values or obvious typos that needed to be fixed. However, looking over the column names and underlying data revealed some modifications needed to be made before modeling. 
```{r, message = FALSE}
library(readr)
parkins = read_csv("parkinsons_updrs.csv")
```

<br>


The "\`subject#\`" column is a unique identifier for each individual. Because our aim is to develop a model that is generally applicable, and not fit to specific individuals of this 42-person study, we will remove it.

```{r}
parkins = subset(parkins, select = -c(`subject#`))
```
```{r, echo=FALSE}
parkins_orig = parkins
```
<br>

#### Variable Examination

Let's take a quick look at the distributions of all of the predictors in the dataset:

```{r eval = FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
library(purrr)
library(tidyr)
library(ggplot2)

parkins %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```


We see right away that the individuals range in age from about 35-90, with the majority of them falling between 55-80 years old.

<br>

There are also many columns with `Jitter` or `Shimmer` in their names, so we suspect there will be some collinearity among them. In addition, since the `motor_UPDRS` score is factored into the `total_UPDRS` score, they are expected to be highly correlated with each other. [Figures 1-4](#collinearity-scatterplot-matrix) show scatterplots of all the columns against our response, `total_UPDRS`, as well as with columns where we suspect collinearity. Predictor columns highly correlated with other predictors are collinear and should likely be removed, but predictor columns highly correlated with our response variable are likely to improve the model.
```{r eval=FALSE, fig.height=5, fig.width=10}
library(GGally)
ggpairs(columns = c(5, 4, 1, 2, 3), data=parkins)
ggpairs(columns = c(5, 17, 18, 19, 20, 21), data=parkins)
ggpairs(columns = c(5, 6, 7, 8, 9, 10), data=parkins)
ggpairs(columns = c(5, 11, 12, 13, 14, 15, 16), data=parkins)
```

Another method of measuring collinearity is through the variance inflation factors (VIF) of the individual predictor columns with each other, where values larger than `10` imply a signficant collinearity. 
```{r, eval=FALSE}
library(faraway)
faraway::vif(parkins)
```

An interesting property of VIF is that if two columns are collinear, their VIF values will be very large. However, if one of those columns is removed and the VIF is recalculated, the leftover column's VIF value will drop significantly if there are no other columns collinear with it. With this, we can take an iterative approach and repeatedly remove the column with the largest VIF until the resulting columns all have values less than `10` or `5`.
```{r}
# Drop columns that have a vif > 5, starting with highest first
fix_vif = function (data) {
  newData = data
  max_vif = max(faraway::vif(newData))
  while (max_vif > 5) {
    #print(summary(lm(total_UPDRS ~ ., data = newData))$adj)
    newData = subset(newData, select = c(setdiff(names(newData), names(which.max(faraway::vif(newData))))))
    max_vif = max(faraway::vif(newData))
  }
  newData
}
```

Evaluating our measures of collinearity, `motor_UPDRS` was found to be highly collinear with `total_UPDRS`, as expected. It makes sense that knowledge of a subject's `motor_UPDRS` score would be incredibly informative for calculating their total score, but a predictive model likely wouldn't be needed if that information was already available, so we chose to remove it from our model building process.
```{r}
parkins = subset(parkins, select = -c(motor_UPDRS))
```

Evaluating the other columns did not show any strong linear correlations with `total_UPDRS`, unfortunately, so there was no early indication that an additive model would be enough. On the other hand, there also were no obvious polynomial relationships, so we felt confident we would not need to apply any polynomial transformations. Interactions were still a strong possibility that couldn't be ignored, especially with `sex` being a factor variable.

### Model Building Phase

To get a general idea of how effective the predictors would be, we used all the rows and all the columns to train fully additive and interaction models. 
```{r}
additive_fit = lm(total_UPDRS ~ . , data = parkins)
interaction_fit = lm(total_UPDRS ~ (.)^2 , data = parkins)
```
- The fully additive model created a poorly fit model with an adjusted R^2 of `r summary(additive_fit)$adj.r.squared`. 
- The interaction model gave a slight improvement, with an adjusted R^2 of `r summary(interaction_fit)$adj.r.squared`.


```{r, message = FALSE, warning = FALSE, include = FALSE}

library(lmtest)

# Pull the helper functions into the current environment
source('helpers.r', chdir = T)
```


<br>

So, anyway, here's a simulation...
```{r message=FALSE, warning=FALSE}
set.seed(420)
num_sims = 1
num_metrics = 8

additive_evals = rep(list(rep(0, num_sims)), num_metrics)
interaction_evals = rep(list(rep(0, num_sims)), num_metrics)
additive_vif_evals = rep(list(rep(0, num_sims)), num_metrics)
interaction_vif_evals = rep(list(rep(0, num_sims)), num_metrics)
ob_fit_evals = rep(list(rep(0, num_sims)), num_metrics)
back_aic_evals = rep(list(rep(0, num_sims)), num_metrics)
back_bic_evals = rep(list(rep(0, num_sims)), num_metrics)

for (i in 1:num_sims) {
  # create a new train/test split
  park_trn_idx  = sample(nrow(parkins), size = trunc(0.80 * nrow(parkins)))
  park_trn_data = parkins[park_trn_idx, ]
  park_tst_data = parkins[-park_trn_idx, ]
  
  # train full additive model
  add_fit = lm(total_UPDRS ~ . , data = park_trn_data)
  add_eval = eval_model(add_fit, park_tst_data)
  
  # train full interaction model
  int_fit = lm(total_UPDRS ~ .^2 , data = park_trn_data)
  int_eval = eval_model(int_fit, park_tst_data)
  
  # use VIF to remove collinear columns
  vif_rem_park_trn_data = fix_vif(park_trn_data)
  
  # train additive model with columns removed by VIF
  add_vif_fit = lm(total_UPDRS ~ . , data = vif_rem_park_trn_data)
  add_vif_eval = eval_model(add_vif_fit, park_tst_data)
  
  # train interaction model with columns removed by VIF
  int_vif_fit = lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data)
  int_vif_eval = eval_model(int_vif_fit, park_tst_data)
  
  
  # OB best model
  ob_init_fit = lm(total_UPDRS ~ ( . - `Jitter(%)` - `Jitter:RAP` - `Shimmer:APQ3`) ^ 2, 
                   data = park_trn_data)
  ob_fit = lm(total_UPDRS ~ ( . - `Jitter(%)` - `Jitter:RAP` - `Shimmer:APQ3`) ^ 2, 
              data = park_trn_data, subset = non_influential_filter(ob_init_fit))
  ob_eval = eval_model(ob_fit, park_tst_data)
  
  # Removing Influential Points, Interaction model, and step back AIC
  back_aic_fit = step(lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data, 
                         subset = non_influential_filter(int_vif_fit)), direction = 'backward', trace = 0)
  back_aic_eval = eval_model(back_aic_fit, park_tst_data)
  
  # Removing Influential Points, Interaction model, and step back BIC
  n = length(resid(int_vif_fit))
  back_bic_fit = step(lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data, 
                         subset = non_influential_filter(int_vif_fit)), direction = 'backward', 
                      trace = 0, k = log(n))
  back_bic_eval = eval_model(back_bic_fit, park_tst_data)
  
  for (j in 1:num_metrics) {
    additive_evals[[j]][i] = add_eval[j]
    interaction_evals[[j]][i] = int_eval[j]
    additive_vif_evals[[j]][i] = add_vif_eval[j]
    interaction_vif_evals[[j]][i] = int_vif_eval[j]
    ob_fit_evals[[j]][i] = ob_eval[j]
    back_aic_evals[[j]][i] = back_aic_eval[j]
    back_bic_evals[[j]][i] = back_bic_eval[j]
  }
  
}

```




Interaction Models

QQplots

Leverage and Influential Points

Collinearity





Model evaluation metrics




## Results

### Column Distributions
```{r echo=TRUE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
library(purrr)
library(tidyr)
library(ggplot2)

parkins %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

<span id="collinearity-scatterplot-matrix"></span>

### Colinearity scatterplot matrix

```{r eval=TRUE, echo=FALSE, fig.align='center', fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
library(GGally)
ggpairs(columns = c(5, 4, 2, 3, 1), data=parkins_orig)
ggpairs(columns = c(5, 17, 18, 19, 20, 21), data=parkins_orig)
ggpairs(columns = c(5, 6, 7, 8, 9, 10), data=parkins_orig)
ggpairs(columns = c(5, 11, 12, 13, 14, 15, 16), data=parkins_orig)
```

### Table of best models, with metrics averaged over 5 data splits

```{r echo = FALSE, fig.align = 'center'}

dframe = data.frame(
  model = c("additive_full", "additive_vif", "interaction", "interaction_vif", "ob_best_model", "back_aic_interaction", "back_bic_interaction"),
  
  b_pagan = round(c(mean(unlist(additive_evals[1])), mean(unlist(additive_vif_evals[1])), mean(unlist(interaction_evals[1])), mean(unlist(interaction_vif_evals[1])), mean(unlist(ob_fit_evals[1])), mean(unlist(back_aic_evals[1])), mean(unlist(back_bic_evals[1]))), 3),
  
  shap_wilk = round(c(mean(unlist(additive_evals[2])), mean(unlist(additive_vif_evals[2])), mean(unlist(interaction_evals[2])), mean(unlist(interaction_vif_evals[2])), mean(unlist(ob_fit_evals[2])), mean(unlist(back_aic_evals[2])), mean(unlist(back_bic_evals[2]))), 3),
  
  rmse_loocv = round(c(mean(unlist(additive_evals[3])), mean(unlist(additive_vif_evals[3])), mean(unlist(interaction_evals[3])), mean(unlist(interaction_vif_evals[3])), mean(unlist(ob_fit_evals[3])), mean(unlist(back_aic_evals[3])), mean(unlist(back_bic_evals[3]))), 3),
  
  rmse_trn = round(c(mean(unlist(additive_evals[4])), mean(unlist(additive_vif_evals[4])), mean(unlist(interaction_evals[4])), mean(unlist(interaction_vif_evals[4])), mean(unlist(ob_fit_evals[4])), mean(unlist(back_aic_evals[4])), mean(unlist(back_bic_evals[4]))), 3),
  
  rmse_tst = round(c(mean(unlist(additive_evals[5])), mean(unlist(additive_vif_evals[5])), mean(unlist(interaction_evals[5])), mean(unlist(interaction_vif_evals[5])), mean(unlist(ob_fit_evals[5])), mean(unlist(back_aic_evals[5])), mean(unlist(back_bic_evals[5]))), 3),
  
  mae_tst = round(c(mean(unlist(additive_evals[8])), mean(unlist(additive_vif_evals[8])), mean(unlist(interaction_evals[8])), mean(unlist(interaction_vif_evals[8])), mean(unlist(ob_fit_evals[8])), mean(unlist(back_aic_evals[8])), mean(unlist(back_bic_evals[8]))), 3),
  
  adj_r2 = round(c(mean(unlist(additive_evals[6])), mean(unlist(additive_vif_evals[6])), mean(unlist(interaction_evals[6])), mean(unlist(interaction_vif_evals[6])), mean(unlist(ob_fit_evals[6])), mean(unlist(back_aic_evals[6])), mean(unlist(back_bic_evals[6]))), 3),
  
  num_pred = round(c(mean(unlist(additive_evals[7])), mean(unlist(additive_vif_evals[7])), mean(unlist(interaction_evals[7])), mean(unlist(interaction_vif_evals[7])), mean(unlist(ob_fit_evals[7])), mean(unlist(back_aic_evals[7])), mean(unlist(back_bic_evals[7]))), 1)
)

knitr::kable(dframe)
```

### QQ Plots and Fitted vs Residuals
```{r, echo=FALSE, fig.align='center'}
park_trn_idx  = sample(nrow(parkins), size = trunc(0.80 * nrow(parkins)))
park_trn_data = parkins[park_trn_idx, ]
park_tst_data = parkins[-park_trn_idx, ]

vif_rem_park_trn_data = fix_vif(park_trn_data)
int_vif_fit = lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data)
  int_vif_eval = eval_model(int_vif_fit, park_tst_data)

back_aic_fit = step(lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data, 
                         subset = non_influential_filter(int_vif_fit)), direction = 'backward', trace = 0)

diagnostic_plots(back_aic_fit)
```

#### Predicted vs Actual plot
```{r, echo=FALSE, fig.align='center'}
plot_pva(predict(back_aic_fit, park_tst_data), park_tst_data$total_UPDRS)
```


## Discussion

### Colinearity among columns

After the results, we found the hight collinearity that exists between the predictors: Jitter(%),  Jitter(Abs), Jitter:RAP,  Jitter:PPQ5, Jitter:DDP,  Shimmer, Shimmer(dB),  Shimmer:APQ3, Shimmer:APQ5, Shimmer:APQ11, Shimmer:DDA and the response total_UPDRS. The collinearity that exists in our dataset could be a cause of low precision of the estimate coefficients. Also, it makes us suspected about the p-values obtained in models where these variables were present together. But after we simulate a model with no collinearity variables, we got lower results in the Adjusted R2 and highest  RMSE values. 

### Influential points

The influential points were key in the selection of our model. We found between 166 to 200 influential points, which is, on average, the 3.9 % of training data set between the different models we fitted. The results obtained were substantially better in the models without influential points.

### Overfitting

We also try different approaches where we got overfitted models. We fitted a model using the AIC backward with Motor__UPDRS variable included. The results were almost perfect. We got an adjusted R2 close to 1 and a low RMSE, but after where plotted the predicted values vs. the actual, we find out that the models were overfitted.

Why we chose the metrics we did

(Comparison with model from paper)



## Appendix

```{r, comment = NA}
# Embed helper functions into document
writeLines(readLines('helpers.r'))
```

