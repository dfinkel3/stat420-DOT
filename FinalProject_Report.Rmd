---
title: "STAT 420: Final Project"
author: "Daniil Finkel, Omar Boffil, Albert Ferguson"
date: "August 7, 2020"
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


<h2>Predicting UPDRS (Unified Parkinson Disease Rating Scale) from Bio-medical Voice Measurements of Individuals with Early Stage Parkinson’s Disease</h2>

<br>

## Introduction

Telehealth is a growing field. Remote medical examinations offer several benefits over conventional on-site methods:

- Secure and confidential: patients do not need to be seen at a specialists office
- Convenience: patients can attend appointments from comfort of their own home
- Increased access: patients are no longer restricted to geographically local physicians
- Reduced cancellation: patients more consistently attend, more closely monitored
- Many others

*Telemonitoring* is one form of telehealth in which information technology is used to monitor patients at a distance. The feasibility of such automated approaches calls for robust diagnoses to justify widespread adoption. This project aims to contribute to that goal. 

Parkinson’s disease is a neurodegenerative disease characterized by stiffness and shakiness in motor functions of affected individuals. The disease tends to become more severe over time.

A research collaboration between the University of Oxford, 10 medical centers in the US, and Intel Corporation organized a 6-month trial, recording a range of bio-medical voice measurements from 42 people with early-stage Parkinson's disease. The effort produced a dataset with the following characteristics:

- 5875 observations
- 22 attributes:
  - subject# - Integer that uniquely identifies each subject
  - age - Subject age
  - sex - Subject gender '0' - male, '1' - female
  - test_time - Time since recruitment into the trial. The integer part is the number of days since recruitment.
  - motor_UPDRS - Clinician's motor UPDRS score, linearly interpolated
  - total_UPDRS - Clinician's total UPDRS score, linearly interpolated
  - Jitter(%), Jitter(Abs), Jitter:RAP, Jitter:PPQ5, Jitter:DDP
    - Several measures of variation in fundamental frequency
  - Shimmer, Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, Shimmer:APQ11, Shimmer:DDA
    - Several measures of variation in amplitude
  - NHR, HNR - Two measures of ratio of noise to tonal components in the voice
  - RPDE - A nonlinear dynamical complexity measure
  - DFA - Signal fractal scaling exponent
  - PPE - A nonlinear measure of fundamental frequency variation

The dataset and a description of the aims of its collection are provided here: https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring

A key attribute in the above set is `total_UPDRS`. The Unified Parkinson's Disease Rating Scale (UPDRS) is used to measure the degree of Parkinson’s in an individual - a higher score indicating a more severe case.

This project aims to develop a model to predict `total_UPDRS` using the remaining attributes, effectively developing a model to predict the degree to which an individual is affected by Parkinson’s disease.

<br>

**Let's take a closer look and do a little bit of pre-processing on the data:**

<br>

## Methods

<br>

### Exploratory Phase

Examining the dataset revealed it was already very clean. There were no empty values or obvious typos that needed to be fixed. However, looking over the column names and underlying data revealed some modifications needed to be made before modeling. 
```{r, message = FALSE}
library(readr)
parkins = read_csv("parkinsons_updrs.csv")
```

<br>


The "\`subject#\`" column is a unique identifier for each individual. Because our aim is to develop a model that is generally applicable, and not fit to specific individuals of this 42-person study, we will remove it.

```{r}
parkins = subset(parkins, select = -c(`subject#`))
```
```{r, echo=FALSE}
parkins_orig = parkins
```
<br>

#### Variable Examination

An evaluation of the distributions of all of the predictors in the dataset can be found in [Figure 1](#column-distributions):

```{r eval = FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
library(purrr)
library(tidyr)
library(ggplot2)

parkins %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```


We see right away that the individuals range in age from about 35-90, with the majority of them falling between 55-80 years old.

<br>

We can also create box plots for these columns, to see a more quantified breakdown of the distribution of values within each. [Figures 2-5](#box-plots) show the results of these plots.

```{r, eval=FALSE, message=FALSE, warning=FALSE}
library(reshape)
library(ggplot2)

show_box_plot = function(frame) {
  meltData = melt(frame)
  p <- ggplot(meltData, aes(variable, value)) 
  p + geom_boxplot() + facet_wrap(~variable, scale="free")
}

show_box_plot(
  data.frame(
    age = parkins$`age`,
    test_time = parkins$`test_time`,
    motor_UPDRS = parkins$`motor_UPDRS`,
    total_UPDRS = parkins$`total_UPDRS`,
	  DFA = parkins$`DFA`
  )
)

show_box_plot(
  data.frame(
    Jitter_Abs = parkins$`Jitter(Abs)`,
    Jitter_RAP = parkins$`Jitter:RAP`,
    Jitter_PPQ5 = parkins$`Jitter:PPQ5`,
    Jitter_DDP = parkins$`Jitter:DDP`,
    Shimmer = parkins$`Shimmer`
  )
)

show_box_plot(
  data.frame(
 	  Shimmer_dB = parkins$`Shimmer(dB)`,
    Shimmer_APQ3 = parkins$`Shimmer:APQ3`,
    Shimmer_APQ5 = parkins$`Shimmer:APQ5`,
    Shimmer_APQ11 = parkins$`Shimmer:APQ11`,
    Shimmer_DDA = parkins$`Shimmer:DDA`
  )
)

show_box_plot(
  data.frame(
    NHR = parkins$`NHR`,
    HNR = parkins$`HNR`,
    RPDE = parkins$`RPDE`,
    Jitter_Percent = parkins$`Jitter(%)`,
    PPE = parkins$`PPE`
  )
)
```

<br>

Figure 2, with the exception of the Age plot, shows plots that look very good at first glance. The Age plot shows a single outlier, a value smaller than the first quartile by more than 1.5 times the *interquartile range* of the data:

$$a \, \epsilon \, Age \, | \, Q1 - a > 1.5 * IQR(Age)$$
The remaining 3 sets of plots show much more interesting data. All three sets of plots show a slim interquartile range, with dozens of outliers. In some cases, the most extreme of which is Jitter_Percent, the outliers stretch to up to 10 times the interquartile range of the data. We anticipate the scale and large number of outliers in a majority of these columns will be problematic during model building. However, because we have no reason to think these data points were collected in error, we will defer correction of these data points to a further step - namely, in the correction of influential values.

<br>

Because the `Jitter` and `Shimmer` variables contain many variations, we assume they share fundamental characteristics and we suspect there will be some collinearity among them. In addition, since the `motor_UPDRS` score is a component in the calculation of the `total_UPDRS` score, we also expect these columns to be highly correlated with one another. [Figures 6-9](#collinearity-scatterplot-matrix) show scatterplots of all the columns against our response, `total_UPDRS`, as well as with columns where we suspect collinearity. Because collinearity can increase the variance of coefficient estimates, making estimates very sensitive to minor changes in our models, we will seek to eliminate collinearity from the set of predictors. By removing collinear relationships between our predictors, we presume we will be building more robust models.

```{r eval=FALSE, fig.height=5, fig.width=10}
library(GGally)
ggpairs(columns = c(5, 4, 1, 2, 3), data=parkins)
ggpairs(columns = c(5, 17, 18, 19, 20, 21), data=parkins)
ggpairs(columns = c(5, 6, 7, 8, 9, 10), data=parkins)
ggpairs(columns = c(5, 11, 12, 13, 14, 15, 16), data=parkins)
```

<br>

Another method of measuring collinearity is through the variance inflation factors (VIF) of the individual predictor columns with each other, where values larger than `5` imply collinearity. 
```{r, eval=FALSE}
library(faraway)
faraway::vif(parkins)
```

<br>

An interesting property of VIF is that if two columns are collinear, their VIF values will be very large. However, if one of those columns is removed and the VIF is recalculated, the leftover column's VIF value will drop significantly if there are no other columns collinear with it. With this, we can take an iterative approach and repeatedly remove the column with the largest VIF until the resulting columns all have values less than `5`.
```{r}
# Drop columns that have a vif > 5, starting with highest first
fix_vif = function (data) {
  newData = data
  max_vif = max(faraway::vif(newData))
  while (max_vif > 5) {
    newData = subset(newData, 
                     select = c(setdiff(names(newData), names(which.max(faraway::vif(newData))))))
    max_vif = max(faraway::vif(newData))
  }
  newData
}
```

<br>

Evaluating our measures of collinearity, `motor_UPDRS` was found to be highly collinear with `total_UPDRS`, as expected. It makes sense that knowledge of a subject's `motor_UPDRS` score would be incredibly informative for calculating their total score, but a predictive model likely wouldn't be needed if that information was already available, so we chose to remove it from our model building process.
```{r}
# Remove the motor_UDPRS column from the dataset
parkins = subset(parkins, select = -c(motor_UPDRS))
```

<br>

Evaluating the other columns did not show any strong linear correlations with `total_UPDRS`, unfortunately, so there was no early indication that an additive model would be enough. On the other hand, there also were no obvious polynomial relationships, so we felt confident we would not need to apply any polynomial transformations. Interactions were still a strong possibility that couldn't be ignored, especially with `sex` being a factor variable.

### Model Building Phase

To get a general idea of how effective the predictors would be, we used all the rows and all the columns to train fully additive and interaction models. 
```{r}
additive_fit = lm(total_UPDRS ~ . , data = parkins)
interaction_fit = lm(total_UPDRS ~ (.)^2 , data = parkins)
```
- The fully additive model created a poorly fit model with an adjusted $R^2$ of ``r summary(additive_fit)$adj.r.squared``. 
- The interaction model gave a slight improvement, with an adjusted $R^2$ of ``r summary(interaction_fit)$adj.r.squared``.


```{r, message = FALSE, warning = FALSE, include = FALSE}

library(lmtest)

# Pull the helper functions into the current environment
source('helpers.r', chdir = T)
```


<br>

So, anyway, here's a simulation...
```{r message=FALSE, warning=FALSE}
set.seed(420)
num_sims = 1
num_metrics = 8

additive_evals = rep(list(rep(0, num_sims)), num_metrics)
interaction_evals = rep(list(rep(0, num_sims)), num_metrics)
additive_vif_evals = rep(list(rep(0, num_sims)), num_metrics)
interaction_vif_evals = rep(list(rep(0, num_sims)), num_metrics)
overfit_evals = rep(list(rep(0, num_sims)), num_metrics)
back_aic_evals = rep(list(rep(0, num_sims)), num_metrics)
back_bic_evals = rep(list(rep(0, num_sims)), num_metrics)

for (i in 1:num_sims) {
  # create a new train/test split
  park_trn_idx  = sample(nrow(parkins), size = trunc(0.80 * nrow(parkins)))
  park_trn_data = parkins[park_trn_idx, ]
  park_tst_data = parkins[-park_trn_idx, ]
  
  # train full additive model
  add_fit = lm(total_UPDRS ~ . , data = park_trn_data)
  add_eval = eval_model(add_fit, park_tst_data)
  
  # train full interaction model
  int_fit = lm(total_UPDRS ~ .^2 , data = park_trn_data)
  int_eval = eval_model(int_fit, park_tst_data)
  
  # use VIF to remove collinear columns
  vif_rem_park_trn_data = fix_vif(park_trn_data)
  
  # train additive model with columns removed by VIF
  add_vif_fit = lm(total_UPDRS ~ . , data = vif_rem_park_trn_data)
  add_vif_eval = eval_model(add_vif_fit, park_tst_data)
  
  # train interaction model with columns removed by VIF
  int_vif_fit = lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data)
  int_vif_eval = eval_model(int_vif_fit, park_tst_data)
  
  
  # model that "overfit"
  overfit_init_fit = lm(total_UPDRS ~ ( . - `Jitter(%)` - `Jitter:RAP` - `Shimmer:APQ3`) ^ 2, 
                   data = park_trn_data)
  overfit_fit = lm(total_UPDRS ~ ( . - `Jitter(%)` - `Jitter:RAP` - `Shimmer:APQ3`) ^ 2, 
              data = park_trn_data, subset = non_influential_filter(overfit_init_fit))
  overfit_eval = eval_model(overfit_fit, park_tst_data)
  
  # Removing Influential Points, Interaction model, and step back AIC
  back_aic_fit = step(lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data, 
                         subset = non_influential_filter(int_vif_fit)), direction = 'backward', trace = 0)
  back_aic_eval = eval_model(back_aic_fit, park_tst_data)
  
  # Removing Influential Points, Interaction model, and step back BIC
  n = length(resid(int_vif_fit))
  back_bic_fit = step(lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data, 
                         subset = non_influential_filter(int_vif_fit)), 
                      direction = 'backward', trace = 0, k = log(n))
  back_bic_eval = eval_model(back_bic_fit, park_tst_data)
  
  for (j in 1:num_metrics) {
    additive_evals[[j]][i] = add_eval[j]
    interaction_evals[[j]][i] = int_eval[j]
    additive_vif_evals[[j]][i] = add_vif_eval[j]
    interaction_vif_evals[[j]][i] = int_vif_eval[j]
    overfit_evals[[j]][i] = overfit_eval[j]
    back_aic_evals[[j]][i] = back_aic_eval[j]
    back_bic_evals[[j]][i] = back_bic_eval[j]
  }
  
}

```

<br>

## Results

<br>

<span id="column-distributions"></span>

#### Column Distributions
```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Figure 1: Column Distributions"}
library(purrr)
library(tidyr)
library(ggplot2)

parkins %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

<br>

In these distributions, we see some familiar shapes. HNR looks to most closely resemble a normal distribution amongst the set of predictors, with RPDE also appearing somewhat normal. These two metrics show a single, symmetric, bell-shaped curve. Several columns, including all Jitter metrics, all Shimmer metrics, and NHR show distributions with heavy right tails. 
<span id="collinearity-scatterplot-matrix"></span>

<br>

<span id="box-plots"></span>

#### Box Plots

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Figure 2: Box plots of age, test_time, motor_UPDRS, total_UPDRS, and DFA"}
library(reshape)
library(ggplot2)

show_box_plot = function(frame) {
  meltData = melt(frame)
  p <- ggplot(meltData, aes(variable, value)) 
  p + geom_boxplot() + facet_wrap(~variable, scale="free")
}
show_box_plot(
  data.frame(
    age = parkins_orig$`age`,
    test_time = parkins_orig$`test_time`,
    motor_UPDRS = parkins_orig$`motor_UPDRS`,
    total_UPDRS = parkins_orig$`total_UPDRS`,
	  DFA = parkins_orig$`DFA`
  )
)
```
<br>
```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Figure 3: Box plots of Jitter_Abs, Jitter_RAP, Jitter_PPQ5, Jitter_DDP, and Shimmer"}
show_box_plot(
  data.frame(
    Jitter_Abs = parkins_orig$`Jitter(Abs)`,
    Jitter_RAP = parkins_orig$`Jitter:RAP`,
    Jitter_PPQ5 = parkins_orig$`Jitter:PPQ5`,
    Jitter_DDP = parkins_orig$`Jitter:DDP`,
    Shimmer = parkins_orig$`Shimmer`
  )
)
```
<br>
```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Figure 4: Box plots of Shimmer_dB, Shimmer_APQ3, Shimmer_APQ5, Shimmer_APQ11, and Shimmer_DDA"}
show_box_plot(
  data.frame(
 	  Shimmer_dB = parkins_orig$`Shimmer(dB)`,
    Shimmer_APQ3 = parkins_orig$`Shimmer:APQ3`,
    Shimmer_APQ5 = parkins_orig$`Shimmer:APQ5`,
    Shimmer_APQ11 = parkins_orig$`Shimmer:APQ11`,
    Shimmer_DDA = parkins_orig$`Shimmer:DDA`
  )
)
```
<br>
```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Figure 5: Box plots of NHR, HNR, RPDE, Jitter_Percent, and PPE"}
show_box_plot(
  data.frame(
    NHR = parkins_orig$`NHR`,
    HNR = parkins_orig$`HNR`,
    RPDE = parkins_orig$`RPDE`,
    Jitter_Percent = parkins_orig$`Jitter(%)`,
    PPE = parkins_orig$`PPE`
  )
)
```


<br>

<span id="collinearity-scatterplot-matrix"></span>

#### Colinearity scatterplot matrix

```{r eval=TRUE, echo=FALSE, fig.align='center', fig.height=5, fig.width=10, message=FALSE, warning=FALSE, fig.cap="Figure 6: Collinearity between total_UPDRS, motor_UPDRS, sex, test_time, and age"}
library(GGally)
ggpairs(columns = c(5, 4, 2, 3, 1), data=parkins_orig)
```
<br>
```{r eval=TRUE, echo=FALSE, fig.align='center', fig.height=5, fig.width=10, message=FALSE, warning=FALSE, fig.cap="Figure 7: Collinearity between total_UPDRS, NHR, HNR, RPDE, DFA, and PPE"}
library(GGally)
ggpairs(columns = c(5, 17, 18, 19, 20, 21), data=parkins_orig)
```
<br>
```{r eval=TRUE, echo=FALSE, fig.align='center', fig.height=5, fig.width=10, message=FALSE, warning=FALSE, fig.cap="Figure 8: Collinearity between total_UPDRS, Jitter(%), Jitter(Abs), Jitter:RAP, Jitter:PPQ5, and Jitter:DDP"}
library(GGally)
ggpairs(columns = c(5, 6, 7, 8, 9, 10), data=parkins_orig)
```
<br>
```{r eval=TRUE, echo=FALSE, fig.align='center', fig.height=5, fig.width=10, message=FALSE, warning=FALSE, fig.cap="Figure 9: Collinearity between total_UPDRS, Shimmer, Shimmer(db), Shimmer:APQ3, Shimmer:APQ5, Shimmer:APQ11, and Shimmer:DDA"}
library(GGally)
ggpairs(columns = c(5, 11, 12, 13, 14, 15, 16), data=parkins_orig)
```

<br>

<span id="table-of-models"></span>

#### Table of best models, with metrics averaged over 5 data splits

```{r echo = FALSE, fig.align = 'center', fig.cap="Table 1: Comparison of models trained on data"}

dframe = data.frame(
  model = c("additive_full", "additive_vif", "interaction", "interaction_vif", "ob_best_model", "back_aic_interaction", "back_bic_interaction"),
  
  # b_pagan = round(c(mean(unlist(additive_evals[1])), mean(unlist(additive_vif_evals[1])), mean(unlist(interaction_evals[1])), mean(unlist(interaction_vif_evals[1])), mean(unlist(overfit_evals[1])), mean(unlist(back_aic_evals[1])), mean(unlist(back_bic_evals[1]))), 3),
  
  # shap_wilk = round(c(mean(unlist(additive_evals[2])), mean(unlist(additive_vif_evals[2])), mean(unlist(interaction_evals[2])), mean(unlist(interaction_vif_evals[2])), mean(unlist(overfit_evals[2])), mean(unlist(back_aic_evals[2])), mean(unlist(back_bic_evals[2]))), 3),
  
  rmse_loocv = round(c(mean(unlist(additive_evals[3])), mean(unlist(additive_vif_evals[3])), mean(unlist(interaction_evals[3])), mean(unlist(interaction_vif_evals[3])), mean(unlist(overfit_evals[3])), mean(unlist(back_aic_evals[3])), mean(unlist(back_bic_evals[3]))), 3),
  
  rmse_trn = round(c(mean(unlist(additive_evals[4])), mean(unlist(additive_vif_evals[4])), mean(unlist(interaction_evals[4])), mean(unlist(interaction_vif_evals[4])), mean(unlist(overfit_evals[4])), mean(unlist(back_aic_evals[4])), mean(unlist(back_bic_evals[4]))), 3),
  
  rmse_tst = round(c(mean(unlist(additive_evals[5])), mean(unlist(additive_vif_evals[5])), mean(unlist(interaction_evals[5])), mean(unlist(interaction_vif_evals[5])), mean(unlist(overfit_evals[5])), mean(unlist(back_aic_evals[5])), mean(unlist(back_bic_evals[5]))), 3),
  
  mae_tst = round(c(mean(unlist(additive_evals[8])), mean(unlist(additive_vif_evals[8])), mean(unlist(interaction_evals[8])), mean(unlist(interaction_vif_evals[8])), mean(unlist(overfit_evals[8])), mean(unlist(back_aic_evals[8])), mean(unlist(back_bic_evals[8]))), 3),
  
  adj_r2 = round(c(mean(unlist(additive_evals[6])), mean(unlist(additive_vif_evals[6])), mean(unlist(interaction_evals[6])), mean(unlist(interaction_vif_evals[6])), mean(unlist(overfit_evals[6])), mean(unlist(back_aic_evals[6])), mean(unlist(back_bic_evals[6]))), 3),
  
  num_pred = round(c(mean(unlist(additive_evals[7])), mean(unlist(additive_vif_evals[7])), mean(unlist(interaction_evals[7])), mean(unlist(interaction_vif_evals[7])), mean(unlist(overfit_evals[7])), mean(unlist(back_aic_evals[7])), mean(unlist(back_bic_evals[7]))), 1)
)

knitr::kable(dframe)
```

<br>

<span id="qq-plots"></span>

#### QQ Plots and Fitted vs Residuals
```{r, echo=FALSE, fig.align='center', fig.cap="Figure 10: Fitted vs Residuals and QQ plot of the best model"}
park_trn_idx  = sample(nrow(parkins), size = trunc(0.80 * nrow(parkins)))
park_trn_data = parkins[park_trn_idx, ]
park_tst_data = parkins[-park_trn_idx, ]

vif_rem_park_trn_data = fix_vif(park_trn_data)
int_vif_fit = lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data)
  int_vif_eval = eval_model(int_vif_fit, park_tst_data)

back_aic_fit = step(lm(total_UPDRS ~ .^2 , data = vif_rem_park_trn_data, 
                         subset = non_influential_filter(int_vif_fit)), direction = 'backward', trace = 0)

diagnostic_plots(back_aic_fit)
```

<br>

<span id="pvas"></span>

#### Predicted vs Actual plot
```{r, echo=FALSE, fig.align='center', fig.cap="Figure 11: Predicted vs Actuals of best model evaluated on test data"}
plot_pva(predict(back_aic_fit, park_tst_data), park_tst_data$total_UPDRS)
```

<br>

## Discussion

#### Colinearity among columns

After the results, we found the high collinearity that exists between the predictors: Jitter(%),  Jitter(Abs), Jitter:RAP,  Jitter:PPQ5, Jitter:DDP,  Shimmer, Shimmer(dB),  Shimmer:APQ3, Shimmer:APQ5, Shimmer:APQ11, Shimmer:DDA and the response total_UPDRS. The collinearity that exists in our dataset could be a cause of low precision of the estimate coefficients. Also, it makes us suspected about the p-values obtained in models where these variables were present together. But after we simulate a model with no collinearity variables, we got lower results in the Adjusted $R^2$ and highest  RMSE values. 

#### Influential points

The influential points were key in the selection of our model. We found between 166 and 200 influential points, which is, on average, the 3.9 % of training data set between the different models we fitted. The results obtained were substantially better in the models without influential points.

#### Overfitting

We also try different approaches where we got overfitted models. We fitted a model using the AIC backward with Motor__UPDRS variable included. The results were almost perfect. We got an adjusted R2 close to 1 and a low RMSE, but after where plotted the predicted values vs. the actual, we find out that the models were overfitted.

Why we chose the metrics we did

(Comparison with model from paper)



## Appendix

```{r, comment = NA}
# Embed helper functions into document
writeLines(readLines('helpers.r'))
```

